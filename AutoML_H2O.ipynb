{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa03252-390b-469f-aa60-fb4f1f289ec1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# H2O AutoML approach\n",
    "## The following code can be used in an Jupyter Notebook (Python 3.8.X, H2O cluster version 3.36.0.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9e751-8c89-4942-a673-eaf1be031832",
   "metadata": {},
   "source": [
    "__Import the required modules.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a23b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a0d5a",
   "metadata": {},
   "source": [
    "Attempting to start a local H2O server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99365901",
   "metadata": {},
   "outputs": [],
   "source": [
    "Read the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'MetS Train.csv'\n",
    "dataframe = read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353e47f",
   "metadata": {},
   "source": [
    "Data preprocessing: filling missing values, substitution of values, select the training features and target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = dataframe.values\n",
    "ID_train = array[:,0]\n",
    "y_train = array[:,-1]\n",
    "htrain = h2o.H2OFrame(dataframe)\n",
    "htrain['Metabolic syndrome'] = htrain['Metabolic syndrome'].asfactor()\n",
    "x = htrain.columns\n",
    "y = 'Metabolic syndrome'\n",
    "x.remove(y)\n",
    "x.remove('Nr_Crt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b80b0-e27b-421b-91da-acbcee674eb6",
   "metadata": {},
   "source": [
    "__Model selection and tuning__. The time limit for running AutoML is set to fifteen minutes. In this scenario we removed algorithms like Stacked Ensemble and Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aml = H2OAutoML(max_models = 3, max_runtime_secs=900, exclude_algos=['StackedEnsemble','DeepLearning'], seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b4cf5",
   "metadata": {},
   "source": [
    "__Training H2O AutoML__. The AutoML leaderboard uses cross-validation metrics to rank the models. The leader model is stored at _aml.leader_ and the leaderboard is stored at _aml.leaderboard_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba72b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aml.train(x=x, y=y, training_frame=htrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e10d40",
   "metadata": {},
   "source": [
    "Checking the Leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb32362",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = aml.leaderboard\n",
    "lb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284f802",
   "metadata": {},
   "source": [
    "__Save the best model to filesystem__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dded56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = h2o.save_model(aml.leader, path = \"h2o_model\")\n",
    "print(model_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ee16d",
   "metadata": {},
   "source": [
    "__H2O Explainability interface__ is a convenient wrapper to a number of explainabilty methods and visualizations in H2O. The _explain()_ function generates a list of explanations â€“ individual units of explanation such as a Partial Dependence plot, a Feature Importance plot or a SHapley Additive exPlanations (SHAP) Summary of Top Tree-based Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c2dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xplain_model = aml.leader.explain(htrain)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "196c406d-5c31-4565-8c73-5306deff3175",
   "metadata": {},
   "source": [
    "Explain a particular model from the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43e6d6-2da0-491e-abb1-42305c883aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost = aml.get_best_model('xgboost')\n",
    "xplain_model = xgboost.explain(htrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cfa4b",
   "metadata": {},
   "source": [
    "__Predicting on train data using the leader model__. The predict function outputs predicted classes, as well as the probability estimates for each of the classes (confidence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1632fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_h2o = aml.leader.predict(htrain)\n",
    "pred_pandas=pred_h2o.as_data_frame(use_pandas=True)\n",
    "probs = pred_pandas.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11304232",
   "metadata": {},
   "source": [
    "__Restore the model from the filesystem__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = h2o.load_model(model_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "decb5d77-d705-487d-b8db-2e7750b69f41",
   "metadata": {},
   "source": [
    "Read the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'MetS Test.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "ID_test = array[:,0]\n",
    "y_test = array[:,-1]\n",
    "htest = h2o.H2OFrame(dataframe)\n",
    "htest['Metabolic syndrome'] = htest['Metabolic syndrome'].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927adf57",
   "metadata": {},
   "source": [
    "__Predicting on test data using the saved model__. The predict function outputs predicted classes, as well as the probability estimates for each of the classes (confidence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80690b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_h2o = saved_model.predict(htest)\n",
    "pred_pandas=pred_h2o.as_data_frame(use_pandas=True)\n",
    "probs = pred_pandas.values\n",
    "\n",
    "pred_confidence = []\n",
    "y_pred = []\n",
    "for i in range(len(probs)):\n",
    "  y_pred.append(probs[i, 0])\n",
    "  if (probs[i, 0]==0):\n",
    "    pred_confidence.append(probs[i, 1])\n",
    "  else:\n",
    "    pred_confidence.append(probs[i, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds_id = pd.DataFrame(ID_test, columns = [\"ID\"])\n",
    "ds_actual = pd.DataFrame(y_test, columns = [\"ACTUALVALUE\"])\n",
    "ds_pred = pd.DataFrame(y_pred, columns = [\"PREDICTEDVALUE\"])\n",
    "ds_prob = pd.DataFrame(pred_confidence, columns = [\"PREDICTIONCONFIDENCE\"])\n",
    "dataframe = pd.concat([ds_id, ds_actual, ds_pred, ds_prob], axis=1)\n",
    "dataframe.to_csv('h2o_test_pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242dc9f",
   "metadata": {},
   "source": [
    "__Build the confusion matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7500fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab87d7",
   "metadata": {},
   "source": [
    "__Display the confusion matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "fig, ax = plot_confusion_matrix(conf_mat=conf_matrix, figsize=(2, 2), cmap=plt.cm.Greens)\n",
    "plt.xlabel('Predictions', fontsize=11)\n",
    "plt.ylabel('Actuals', fontsize=11)\n",
    "plt.title('Confusion Matrix', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d12823-79cb-4b37-9837-70928b342359",
   "metadata": {},
   "source": [
    "__Calculate the performance metrics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e2c7a-95e8-47b0-9422-0ab07a728104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "print('Precision: %.3f' % precision_score(y_test, y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_test, y_pred))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_pred))\n",
    "print('Specificity : %.3f' %recall_score(y_test, y_pred, pos_label=0))\n",
    "ba = (recall_score(y_test, y_pred, pos_label=0) + recall_score(y_test, y_pred))/2.0\n",
    "print('Balanced accuracy : %.3f' % ba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d493c31-4577-470e-9c8d-22d376b7ba9b",
   "metadata": {},
   "source": [
    "__Prepares the test data for the calculation of SHAP values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2e204-a7bb-4af3-a56a-df3da5189559",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = htest.drop('Nr_Crt').drop('Metabolic syndrome').as_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d9745-9614-4113-9b1d-297a1cc8aa3b",
   "metadata": {},
   "source": [
    "__A wrapper class for H2O models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff8a80-8ac8-40b5-a36e-29337cc508e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2OProbWrapper:\n",
    "    def __init__(self, h2o_model, feature_names):\n",
    "        self.h2o_model = h2o_model\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    def predict_binary_prob(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            X = X.values.reshape(1,-1)\n",
    "        self.dataframe= pd.DataFrame(X, columns=self.feature_names)\n",
    "        self.predictions = self.h2o_model.predict(h2o.H2OFrame(self.dataframe)).as_data_frame().values\n",
    "        return self.predictions.astype('float64')[:,-1] #probability of True class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a7dcf-bd71-4e84-bdbc-b8e48a08c2f8",
   "metadata": {},
   "source": [
    "__SHAP explainer instantiation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755e937-5ae1-4c7f-8c8c-04dd29e3f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_wrapper = H2OProbWrapper(saved_model, X_test.columns)\n",
    "h2o_explainer = shap.KernelExplainer(h2o_wrapper.predict_binary_prob, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261f31e-dd07-452d-a823-a2714bf8b567",
   "metadata": {},
   "source": [
    "__Computing SHAP values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72641ae8-c394-4dde-8027-20cc11ed6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = h2o_explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dea9a5-8a36-4662-9f76-5622aa117786",
   "metadata": {},
   "source": [
    "__Show summary plot__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4d485-5218-447b-90e8-5e041c8b4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values.values, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
